# -*- coding: utf-8 -*-
"""Growing # of Spikes Numerics 2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-_bY7v5peX4vnciqKuJZSi43SkCKwxc6
"""

#imports
import random
import scipy
from scipy.integrate import quad
from scipy.optimize import brentq
import numpy as np
from scipy.linalg import svd
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
!pip install TracyWidom
from TracyWidom import TracyWidom

#algorithm reference: https://arxiv.org/pdf/2006.00436

#returns the Marchenko-Pastur distribution with certain gamma and variance
def MPdistribution(gamma, sigma_square):
  lambda_minus = sigma_square * (1 - np.sqrt(gamma)) * (1 - np.sqrt(gamma))
  lambda_plus = sigma_square * (1 + np.sqrt(gamma)) * (1 + np.sqrt(gamma))
  def distribution(x):
    if x <= lambda_minus or x >= lambda_plus:
      return 0
    return np.sqrt((lambda_plus - x) * (x - lambda_minus)) / (2 * np.pi * sigma_square * min(1, gamma) * x)
  return distribution

#calculates q-quantile of the Marchenko-Pastur distribution with certain gamma and variance 1
def MPquantile(gamma, q):
  lambda_minus = (1 - np.sqrt(gamma)) * (1 - np.sqrt(gamma))
  lambda_plus = (1 + np.sqrt(gamma)) * (1 + np.sqrt(gamma))
  distribution = MPdistribution(gamma, 1)
  def equation(x):
    return quad(distribution, lambda_minus, x)[0] - q
  return brentq(equation, lambda_minus, lambda_plus)

#performs BEMA algorithm for the standard spiked covariance model
def BEMA(evals, p, n, alpha, beta):

  p_tilde = min(n, p) #it is also equal to len(evals)
  gamma = p/n

  lower_k = int(np.ceil(alpha * p_tilde))
  upper_k = int(np.floor((1 - alpha) * p_tilde))

  #calculating quantiles
  quantiles = np.empty(upper_k - lower_k + 1)
  for k in range(lower_k, upper_k + 1):
    quantiles[k - lower_k] = MPquantile(gamma, k/p_tilde)

  #calculating sigma
  sigma_numerator = 0
  sigma_denominator = 0

  for k in range(lower_k, upper_k + 1):
    sigma_numerator += quantiles[k - lower_k] * evals[upper_k + lower_k - k]
    sigma_denominator += quantiles[k - lower_k] * quantiles[k - lower_k]

  sigma_square = sigma_numerator / sigma_denominator

  #calculating (1-beta)-quantile of Tracy-Widom distribution
  tw1 = TracyWidom(1)
  t =tw1.cdfinv(1-beta)

  #calculating K
  K = 0

  lambda_plus = sigma_square * (((1 + np.sqrt(gamma)) * (1 + np.sqrt(gamma)) + t * n ** (-2/3) * gamma ** (-1/6) * (1 + np.sqrt(gamma)) ** (4/3)))
  for i in range(evals.size):
    if evals[i] > lambda_plus:
      K += 1

  return K, lambda_plus

#simple custom Neural Network
class NeuralNetwork(nn.Module):

  def __init__(self, layers, criterion):
    super(NeuralNetwork, self).__init__()
    self.layers = nn.ModuleList(layers)
    self.criterion = criterion
    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    self.to(self.device)

  def set_optimizer(self, optimizer):
    self.optimizer = optimizer

  def forward(self, x):
    for layer in self.layers:
      x = layer(x)
    return x

  def train_step(self, train_loader):
    self.train()
    for train_inputs, train_outputs in train_loader:
      train_inputs = train_inputs.to(self.device)
      train_outputs = train_outputs.to(self.device)
      self.optimizer.zero_grad()
      model_outputs = self(train_inputs)
      loss = self.criterion(model_outputs, train_outputs)
      loss.backward()
      assert not torch.isnan(loss).any()
      self.optimizer.step()

  def evaluate(self, test_loader):
    self.eval()
    correct = 0
    total = 0
    with torch.no_grad():
      for test_inputs, test_outputs in test_loader:
        model_outputs = self(test_inputs)
        _, predicted = torch.max(model_outputs, dim = 1)
        total += test_outputs.size(0)
        correct += (predicted == test_outputs).sum().item()
    return correct / total

  def get_linear_layers(self):
    return [layer.weight for layer in self.layers if isinstance(layer, nn.Linear)]

#convenient factory for the simple Neural Network
def getSimpleNN(layers_topology, learning_rate):
  layers = [nn.Flatten()]
  for i in range(len(layers_topology) - 2):
    layers += [nn.Linear(layers_topology[i], layers_topology[i + 1]), nn.ReLU()]
  layers += [nn.Linear(layers_topology[len(layers_topology) - 2], layers_topology[len(layers_topology) - 1])]

  criterion = nn.CrossEntropyLoss()
  model = NeuralNetwork(layers, criterion)
  model.set_optimizer(optim.SGD(model.parameters(), lr=learning_rate))
  return model

# Create a DataLoader for the training data
train_data = pd.read_csv("sample_data/fashion-mnist_train.csv")
train_data_input = torch.tensor(train_data.iloc[:, 1:].values, dtype=torch.float32) / 256
train_data_output = torch.tensor(train_data.iloc[:, 0].values, dtype=torch.long)
train_dataset = TensorDataset(train_data_input, train_data_output)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

test_data = pd.read_csv("sample_data/fashion-mnist_test.csv")
test_data_input = torch.tensor(test_data.iloc[:, 1:].values, dtype=torch.float32) / 256
test_data_output = torch.tensor(test_data.iloc[:, 0].values, dtype=torch.long)
test_dataset = TensorDataset(test_data_input, test_data_output)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)

# Define hyperparameters
alpha = 0.2
beta = 0.1
learning_rate = 0.01

def bigSingVals(matrix):
  with torch.no_grad():
    _, S, _ = svd(matrix)
    evals = S * S / matrix.shape[1]
    ans, _ = BEMA(evals, matrix.shape[0], matrix.shape[1], alpha, beta)
    return ans

matrix_sizes = list(range(200, 201, 200)) #size of the middle matrix
num_of_spikes = np.zeros(len(matrix_sizes)) #number of spikes after training
trained_accuracy = np.zeros(len(matrix_sizes)) #accuracy after training

layers_topology = [[784, i, i, 10] for i in matrix_sizes]
epochs = 1

for i in range(len(layers_topology)):
  model = getSimpleNN(layers_topology[i], learning_rate)
  for epoch in range(epochs):
    model.train_step(train_loader)
  trained_accuracy[i] = model.evaluate(test_loader)
  num_of_spikes[i] = bigSingVals(model.get_linear_layers()[1])

plt.plot(matrix_sizes, num_of_spikes)
plt.xlabel('Matrix size')
plt.ylabel('Number of spikes')
plt.show()

plt.plot(matrix_sizes, trained_accuracy)
plt.xlabel('Matrix size')
plt.ylabel('Accuracy')
plt.show()

#save data
data = np.array([matrix_sizes, num_of_spikes, trained_accuracy])
np.save("data.npy", data)